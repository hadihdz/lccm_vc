# A PyTorch implementation of LCCM-VC: Learned Conditional Coding Modes For Video Coding
## Original Paper:
H. Hadizadeh and I. V. Bajic, "LCCM-VC: Learned Conditional Coding Modes For Video Coding"

## Installation:
Note: LCCM-VC is built on CANF-VC [https://github.com/NYCU-MAPL/CANF-VC].
1. Clone CANF-VC from https://github.com/NYCU-MAPL/CANF-VC
2. Create a conda environment and install the requirements for installing CANF-VC as instructed at https://github.com/NYCU-MAPL/CANF-VC
3. Copy eval_lccm_vc.py to the main directory of CANF-VC.

## Checkpoints:
The checkpoints for LCCM-VC are available via the following Google Drive link:
https://drive.google.com/drive/folders/1fYlJJCC9EoSr2zm5zHVsnfRt3qK6zrNq?usp=sharing

## Datasets
To evaluate CANF-VC on a video dataset, follow the procedure mentioned for CANF-VC [https://github.com/NYCU-MAPL/CANF-VC] to prepare the datasets. 

## Evaluation:
The testing procedure for LCCM-VC is the same as CANF-VC. 
